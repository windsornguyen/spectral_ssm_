{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb26107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0230355c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Running on device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a818da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mn4560/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/mn4560/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/mn4560/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/mn4560/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "REPO_NAME = \"facebookresearch/dinov2\"\n",
    "MODEL_NAME = \"dinov2_vits14_reg\"\n",
    "model = torch.hub.load(REPO_NAME, 'dinov2_vits14').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90223c04-e7da-4738-bb16-d4f7025aa3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoV2Matcher:\n",
    "    def __init__(\n",
    "    self, \n",
    "    repo_name=REPO_NAME, \n",
    "    model_name=MODEL_NAME, \n",
    "    smaller_edge_size=448, \n",
    "    half_precision=False, \n",
    "    device=\"cuda\"\n",
    "    ):\n",
    "        self.repo_name = repo_name\n",
    "        self.model_name = model_name\n",
    "        self.smaller_edge_size = smaller_edge_size\n",
    "        self.half_precision = half_precision\n",
    "        self.device = device\n",
    "\n",
    "        if self.half_precision:\n",
    "            self.model = torch.hub.load(repo_or_dir=repo_name, model=model_name).half().to(self.device)\n",
    "        else:\n",
    "            self.model = torch.hub.load(repo_or_dir=repo_name, model=model_name).to(self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # Rescale to ImageNet defaults\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(\n",
    "                size=smaller_edge_size, \n",
    "                interpolation=transforms.InterpolationMode.BICUBIC, \n",
    "                antialias=True\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "\n",
    "\n",
    "    # https://github.com/facebookresearch/dinov2/blob/255861375864acdd830f99fdae3d9db65623dafe/notebooks/features.ipynb\n",
    "    def prepare_image(\n",
    "        self, \n",
    "        rgb_image_numpy\n",
    "    ):\n",
    "        image = Image.fromarray(rgb_image_numpy)\n",
    "        image_tensor = self.transform(image)\n",
    "        resize_scale = image.width / image_tensor.shape[2]\n",
    "\n",
    "        # Crop image to dimensions that are a multiple of the patch size\n",
    "        height, width = image_tensor.shape[1:] # -> C x H x W\n",
    "        \n",
    "        # crop a bit from right and bottom parts\n",
    "        cropped_width, cropped_height = width - width % self.model.patch_size, height - height % self.model.patch_size\n",
    "        image_tensor = image_tensor[:, :cropped_height, :cropped_width]\n",
    "\n",
    "        grid_size = (cropped_height // self.model.patch_size, cropped_width // self.model.patch_size)\n",
    "        return image_tensor, grid_size, resize_scale\n",
    "\n",
    "\n",
    "    def prepare_mask(\n",
    "        self, \n",
    "        mask_image_numpy, \n",
    "        grid_size, \n",
    "        resize_scale\n",
    "    ):\n",
    "        cropped_mask_image_numpy = mask_image_numpy[:int(grid_size[0]*self.model.patch_size*resize_scale), :int(grid_size[1]*self.model.patch_size*resize_scale)]\n",
    "        image = Image.fromarray(cropped_mask_image_numpy)\n",
    "        resized_mask = image.resize((grid_size[1], grid_size[0]), resample=Image.Resampling.NEAREST)\n",
    "        resized_mask = np.asarray(resized_mask).flatten()\n",
    "        return resized_mask\n",
    "\n",
    "\n",
    "    def extract_features(\n",
    "        self, \n",
    "        image_tensor\n",
    "    ):\n",
    "        with torch.inference_mode():\n",
    "            if self.half_precision:\n",
    "                image_batch = image_tensor.unsqueeze(0).half().to(self.device)\n",
    "            else:\n",
    "                image_batch = image_tensor.unsqueeze(0).to(self.device)\n",
    "\n",
    "            tokens = self.model.get_intermediate_layers(image_batch)[0].squeeze()\n",
    "            print(tokens)\n",
    "            return tokens.cpu().numpy()\n",
    "\n",
    "\n",
    "    def idx_to_source_position(\n",
    "        self, \n",
    "        idx, \n",
    "        grid_size, \n",
    "        resize_scale\n",
    "    ):\n",
    "        row = (idx // grid_size[1])*self.model.patch_size*resize_scale + self.model.patch_size / 2\n",
    "        col = (idx % grid_size[1])*self.model.patch_size*resize_scale + self.model.patch_size / 2\n",
    "        return row, col\n",
    "\n",
    "\n",
    "    def get_embedding_visualization(\n",
    "        self, \n",
    "        tokens, \n",
    "        grid_size, \n",
    "        resized_mask=None\n",
    "    ):\n",
    "        pca = PCA(n_components=3)\n",
    "        if resized_mask is not None:\n",
    "            tokens = tokens[resized_mask]\n",
    "        reduced_tokens = pca.fit_transform(tokens.astype(np.float32))\n",
    "        if resized_mask is not None:\n",
    "            tmp_tokens = np.zeros((*resized_mask.shape, 3), dtype=reduced_tokens.dtype)\n",
    "            tmp_tokens[resized_mask] = reduced_tokens\n",
    "            reduced_tokens = tmp_tokens\n",
    "        reduced_tokens = reduced_tokens.reshape((*grid_size, -1))\n",
    "        normalized_tokens = (reduced_tokens-np.min(reduced_tokens))/(np.max(reduced_tokens)-np.min(reduced_tokens))\n",
    "        return normalized_tokens\n",
    "\n",
    "\n",
    "    def get_combined_embedding_visualization(\n",
    "        self, \n",
    "        tokens1, \n",
    "        token2, \n",
    "        grid_size1, \n",
    "        grid_size2, \n",
    "        mask1=None, \n",
    "        mask2=None, \n",
    "        random_state=20\n",
    "    ):\n",
    "        pca = PCA(n_components=3, random_state=random_state)\n",
    "        \n",
    "        token1_shape = tokens1.shape[0]\n",
    "        if mask1 is not None:\n",
    "            tokens1 = tokens1[mask1]\n",
    "        if mask2 is not None:\n",
    "            token2 = token2[mask2]\n",
    "        combinedtokens= np.concatenate((tokens1, token2), axis=0)\n",
    "        reduced_tokens = pca.fit_transform(combinedtokens.astype(np.float32))\n",
    "        \n",
    "        \n",
    "        if mask1 is not None and mask2 is not None:\n",
    "            resized_mask = np.concatenate((mask1, mask2), axis=0)\n",
    "            tmp_tokens = np.zeros((*resized_mask.shape, 3), dtype=reduced_tokens.dtype)\n",
    "            tmp_tokens[resized_mask] = reduced_tokens\n",
    "            reduced_tokens = tmp_tokens\n",
    "        elif mask1 is not None and mask2 is None:\n",
    "            return sys.exit(\"Either use both masks or none\")\n",
    "        elif mask1 is None and mask2 is not None:\n",
    "            return sys.exit(\"Either use both masks or none\")\n",
    "        \n",
    "        print(\"tokens1.shape\", tokens1.shape)\n",
    "        print(\"token2.shape\", token2.shape)\n",
    "        print(\"reduced_tokens.shape\", reduced_tokens.shape)\n",
    "        normalized_tokens = (reduced_tokens-np.min(reduced_tokens))/(np.max(reduced_tokens)-np.min(reduced_tokens))\n",
    "\n",
    "        rgbimg1 = normalized_tokens[0:token1_shape,:]\n",
    "        rgbimg2 = normalized_tokens[token1_shape:,:]\n",
    "\n",
    "        rgbimg1 = rgbimg1.reshape((*grid_size1, -1))\n",
    "        rgbimg2 = rgbimg2.reshape((*grid_size2, -1))\n",
    "        return rgbimg1,rgbimg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b470389d-a897-416e-9601-aeacb39cd694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mn4560/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5454,  2.0980,  3.3032,  ...,  2.4588,  2.0603,  3.0550],\n",
      "        [-1.3895,  1.8239,  3.3942,  ...,  1.7078,  2.0420,  2.8918],\n",
      "        [-0.6265,  2.0717,  3.9208,  ...,  0.4604,  2.1315,  2.5929],\n",
      "        ...,\n",
      "        [-0.0944, -1.7360,  3.8943,  ..., -0.0304,  0.8920,  3.4019],\n",
      "        [-0.0931,  0.7979,  3.7121,  ...,  0.9696,  0.8654,  2.9916],\n",
      "        [-0.3459, -0.2272,  2.5107,  ...,  0.9055,  1.2308,  2.6028]],\n",
      "       device='cuda:0')\n",
      "tensor([[-1.6779,  1.1822,  2.1663,  ...,  2.5440,  1.4690,  2.4890],\n",
      "        [-1.7166,  1.0868,  2.7987,  ...,  1.5234,  1.7411,  2.6487],\n",
      "        [-2.6033,  0.5032,  2.5419,  ...,  2.0332,  1.3798,  2.6301],\n",
      "        ...,\n",
      "        [-1.6551, -2.2631,  1.7313,  ...,  1.0665,  0.2895,  3.9159],\n",
      "        [-1.4922, -0.7579,  2.4468,  ...,  1.3105,  0.5018,  3.8212],\n",
      "        [-1.2103, -1.1017,  1.5953,  ...,  1.6000,  0.4576,  3.6525]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m pca_features2 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keypoint1, keypoint2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keypoints1, keypoints2):\n\u001b[0;32m---> 53\u001b[0m     patch1 \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkeypoint1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeypoint1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     54\u001b[0m     patch2 \u001b[38;5;241m=\u001b[39m features2[keypoint2[\u001b[38;5;241m0\u001b[39m], keypoint2[\u001b[38;5;241m1\u001b[39m], :]\n\u001b[1;32m     55\u001b[0m     pca_features \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(np\u001b[38;5;241m.\u001b[39mvstack((patch1, patch2)))\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def get_keypoints(features, grid_size, resize_scale, top_k=10):\n",
    "    # Compute the mean feature vector\n",
    "    mean_feature = np.mean(features, axis=0)\n",
    "\n",
    "    # Compute the L2 distance between each feature vector and the mean feature vector\n",
    "    distances = np.linalg.norm(features - mean_feature, axis=1)\n",
    "\n",
    "    # Get the indices of the top-k salient features\n",
    "    salient_indices = np.argsort(distances)[-top_k:]\n",
    "\n",
    "    # Convert salient indices to keypoint locations\n",
    "    keypoints = [dm.idx_to_source_position(idx, grid_size, resize_scale) for idx in salient_indices]\n",
    "\n",
    "    return keypoints\n",
    "\n",
    "\n",
    "# Load image and mask\n",
    "image1 = cv2.cvtColor(cv2.imread('Images/asd1.jpg', cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "mask1 = cv2.imread('Images/asd1_mask.jpg', cv2.IMREAD_COLOR)[:, :, 0] > 127\n",
    "\n",
    "image2 = cv2.cvtColor(cv2.imread('Images/asd2.jpg', cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "mask2 = cv2.imread('Images/asd2_mask.jpg', cv2.IMREAD_COLOR)[:, :, 0] > 127\n",
    "\n",
    "# Init model\n",
    "dm = DinoV2Matcher(repo_name=REPO_NAME, model_name=MODEL_NAME, half_precision=False)\n",
    "\n",
    "# Extract features\n",
    "image_tensor1, grid_size1, resize_scale1 = dm.prepare_image(image1)\n",
    "features1 = dm.extract_features(image_tensor1)\n",
    "\n",
    "image_tensor2, grid_size2, resize_scale2 = dm.prepare_image(image2)\n",
    "features2 = dm.extract_features(image_tensor2)\n",
    "\n",
    "# Prepare masks\n",
    "resized_mask1 = dm.prepare_mask(mask1, grid_size1, resize_scale1)\n",
    "resized_mask2 = dm.prepare_mask(mask2, grid_size2, resize_scale2)\n",
    "\n",
    "# Get keypoints\n",
    "keypoints1 = get_keypoints(features1, grid_size1, resize_scale1, top_k=300)\n",
    "keypoints2 = get_keypoints(features2, grid_size2, resize_scale2, top_k=300)\n",
    "\n",
    "# Compute PCA between patches of the images\n",
    "pca = PCA(n_components=3)\n",
    "pca_features1 = []\n",
    "pca_features2 = []\n",
    "\n",
    "for keypoint1, keypoint2 in zip(keypoints1, keypoints2):\n",
    "    patch1 = features1[keypoint1[0], keypoint1[1], :]\n",
    "    patch2 = features2[keypoint2[0], keypoint2[1], :]\n",
    "    pca_features = pca.fit_transform(np.vstack((patch1, patch2)))\n",
    "    pca_features1.append(pca_features[0])\n",
    "    pca_features2.append(pca_features[1])\n",
    "\n",
    "pca_features1 = np.array(pca_features1)\n",
    "pca_features2 = np.array(pca_features2)\n",
    "\n",
    "# Normalize PCA features\n",
    "pca_features1 = (pca_features1 - np.min(pca_features1)) / (np.max(pca_features1) - np.min(pca_features1))\n",
    "pca_features2 = (pca_features2 - np.min(pca_features2)) / (np.max(pca_features2) - np.min(pca_features2))\n",
    "\n",
    "# Threshold the first PCA component to remove background\n",
    "threshold = 0.2\n",
    "pca_features1[:, 0] = np.where(pca_features1[:, 0] > threshold, pca_features1[:, 0], 0)\n",
    "pca_features2[:, 0] = np.where(pca_features2[:, 0] > threshold, pca_features2[:, 0], 0)\n",
    "\n",
    "# Visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Plot PCA features for image1\n",
    "ax1.imshow(image1)\n",
    "for keypoint, pca_feature in zip(keypoints1, pca_features1):\n",
    "    color = pca_feature\n",
    "    ax1.plot(keypoint[1], keypoint[0], 'o', markersize=5, color=color)\n",
    "ax1.set_title(\"Image 1 with PCA Features\")\n",
    "\n",
    "# Plot PCA features for image2\n",
    "ax3.imshow(image2)\n",
    "for keypoint, pca_feature in zip(keypoints2, pca_features2):\n",
    "    color = pca_feature\n",
    "    ax3.plot(keypoint[1], keypoint[0], 'o', markersize=5, color=color)\n",
    "ax3.set_title(\"Image 2 with PCA Features\")\n",
    "\n",
    "# Use mask\n",
    "vis_image3, vis_image4 = dm.get_combined_embedding_visualization(\n",
    "    features1, features2, grid_size1, grid_size2, resized_mask1, resized_mask2\n",
    ")\n",
    "\n",
    "ax2.imshow(vis_image3)\n",
    "ax2.set_title(\"Combined Embedding Visualization (Image 1)\")\n",
    "ax4.imshow(vis_image4)\n",
    "ax4.set_title(\"Combined Embedding Visualization (Image 2)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Example ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mmcv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmpose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MMPoseInferencer\n\u001b[1;32m      3\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImages/asd2.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m   \u001b[38;5;66;03m# replace this with your own image path\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# instantiate the inferencer using the model alias\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ssm_env/lib/python3.11/site-packages/mmpose/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) OpenMMLab. All rights reserved.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmmcv\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmmengine\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m digit_version\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmcv'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
